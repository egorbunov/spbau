%!TEX program = xelatex

\input{env.tex}
\makeatletter
\def\input@path{{./figs/}}
\makeatother
\graphicspath{{./figs/}}

\title{Экзаменационные билеты \\ Алгоритмы. 5 курс. Весенний семестр.}
\author{Горбунов Егор Алексеевич}

\begin{document}
\maketitle

\section{Декартово дерево}
\begin{defn}
Декартово дерево -- это бинарное дерево, в каждой вершине которого хранится пара $(k, p)$, причём декартово дерево является деревом поиска по ключу $k$ и кучей по приоритету $p$.
\end{defn}

\begin{lem}
Пускай нам дан набор пар $(k_1, p_1), (k_2, p_2), \ldots, (k_n, p_n)$, причём все $p_i$ различны. Тогда существует единственное декартово дерево, построенное по этому набору пар.
\end{lem}
\begin{proof}
Будем рекурсивно (по индукции) строить декартово дерево. Все приоритеты $p_i$ различны, а значит среди пар можно выбрать \emph{единственную}, у которой приоритет $p_i = p_{max}$ максимальный. Относительно этого приоритета все пары мы можем однозначно разбить на две группы: в <<левую>> группу отправим пары у которых $p_i < p_{max}$, а в правую те, у которых $p_i \geq p_{max}$.
Таким образом разделили исходную задачу размера $n$ на более мелкие две. Построим соответственно декартово дерево на <<левой>> и <<правой>> группе и присоединим к корневой паре с приоритетом $p_{max}$. Дерево построено, значит существует. На каждом уровне выбор корня делается единственным способом (тут используем единственность приоритетов), а значит дерево единственно.
\end{proof}

\begin{note}
Алгоритм, описанный в доказательстве леммы отработает за $\bigO{n^2}$.
\end{note}

\subsection{Построение за линейное время}

Пускай нам дан \emph{уже отсортированный} по ключам в порядке возрастания набор пар $(k_1, p_1), \ldots, (k_n, p_n)$. 
Построим по ним декартово дерево быстро -- за $\bigO{n}$. Декартово дерево -- бинарное дерево поиска по ключам $k_i$, а это значит, что при добавлении ключей в порядке возрастания, добавляемая вершинка должна оказаться самой правой (у нас в левой веточке ключи $< k$, а в правой $\geq k$). Заметим таким образом, что мы всегда можем добавлять новую вершину $x$ в правую ветвь декартова дерева:

\begin{enumerate}[label=\arabic*.]
\item В самой правой ветви найти такие две вершины $a$ и $b$ ($b$ -- непосредственный сын $a$), что $a.p > x.p$ и $x.p > b.p$ (всякие граничные случаи где $a = NIL$ или $b = NIL$ не берём в голову, сейчас пофиг)
\item Перекинуть ссылки на сынишек: $x.l = b,\ a.r = x$
\end{enumerate}

\begin{figure}[ht!]
\centering
\begin{tikzpicture}[scale=1, baseline=(current bounding box.center)]
	\tikzstyle{every node}=[font=\scriptsize]
\begin{scope}[VertexStyle/.append style = {minimum size = 15pt}]
    \Vertex[NoLabel, Math, x=0, y=0]{root}
    \Vertex[Math, x=0.5, y=-1]{a}
    \Vertex[Math, x=1, y=-2]{b}
    \Vertex[NoLabel, Math, x=1.5, y=-3]{c}
\end{scope}
\begin{scope}[VertexStyle/.append style = {minimum size = 15pt, fill=blue!50}]
    \Vertex[Math, x=1.7, y=-1.25]{x}
\end{scope}
\begin{scope}[VertexStyle/.append style = {shape = regular polygon, regular polygon sides = 3}]
	\Vertex[NoLabel, Math, x=-0.5, y=-1]{X}
	\Vertex[NoLabel, Math, x=0, y=-2]{Y}
	\Vertex[NoLabel, Math, x=0.5,y=-3]{Z}
\end{scope}
\foreach \x/\y in {root/a, b/c} {\Edge[style=dashed](\x)(\y)}
\foreach \x/\y in {root/X, a/Y, b/Z} {\Edge(\x)(\y)}
\tikzset{LabelStyle/.style= {fill=none}}
\Edge[label=\tiny{$a.r$}, color=blue!50](a)(x)
\Edge[label=\tiny{$x.l$}, color=blue!50](x)(b)
\tikzset{LabelStyle/.style= {fill=none, text=red}}
\Edge[label=\Large{$\times$}](a)(b)
\end{tikzpicture}
\caption{Вставка вершины $x$ при построении декартова дерева}
\end{figure}

Как находить такие вершины $a$ и $b$? Можно спускаться от корня вниз и вправо. Но тогда представим себе такую ситуацию: на вход подали набор пар $(1, 1), (2, 2), (3, 3), \ldots, (n, n)$. Пары отсортированы как надо. На каждой итерации алгоритма построения дерева новая вершина будет добавляться в самый конец пути вправо, таким образом для нахождения вершины $a$ мы каждый раз будем спускаться вниз проходя по $i$ вершин на $i$-ой итерации. Это как-то квадратично, а мы хотим за $\bigO{n}$.

Будем искать место для вставки с конца правой ветки. Почему это разумно? А потому, что веточка снизу ломается влево каждый раз при добавлении новой вершины там, куда её добавили. Это наводит на разные амортизационные мысли. Имеем дерево $T$, которое строится.
\[
	\Phi_i(T) = \text{длина правой ветки дерева перед $i$-ой итерацией}
\]
Тогда амортизационная стоимость итерации (добавления одной вершинки) равна (тут $t$ --- это то, сколько мы прошли по правой ветки снизу в поисках места вставки)
\[
	\tilde{c_i} = t + \Delta_i \Phi = t + ((l - t + 1) - l) = t - t + 1 = 1 = \bigO{1}
\]
Таким образом амортизационная оценка на построение всего дерева: $\bigO{n}$!
\subsection{Операции вставки и удаления}
Для вставки и удаления введём 2 дополнительные операции: \texttt{Split} и \texttt{Merge}

\begin{itemize}
\item \emph{Процедура \texttt{Split}} принимает на вход дерево $T$ и ключик $k$, а возвращает 2 декартовых дерева: в первом дереве все ключики $< k$, а во втором $\geq k$.
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def Split(T, k):
    if T == nil: return (nil, nil) # nil is empty tree or something like this...
    if T.k < k:
        (T.r, R) = Split(T.r, k)
        return (T, R)
    else:
        (L, T.l) = Split(T.l, k)
        return (L, T)
\end{minted}
Логика проста: если ключ в корне дерева меньше $k$, то этот корень уже находится в искомой <<левой>> половине разбиения (сплита, разреза, как хотите!), а значит <<правую>> половину разбиения мы берём из рекурсивного разбиения правого сына, а левые остатки присоединяем к итоговой левой части.

\item \emph{Процедура \texttt{Merge}} принимает на вход два дерева $L$ и $R$, причём таких, что ключи в первом дереве меньше либо равны ключам во втором и возвращает декартово дерево содержащее ключи обоих деревьев. Опять просто и рекурсивно:
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def Merge(L, R):
	if size(L) == 0: return R
	if size(R) == 0: return L
	if L.p > R.p:
	    L.r = Merge(L.r, R)
	    return L
	else:
	    R.l = Merge(L, R.l)
	    return R
\end{minted}
Тут мы просто уменьшаем задачу валидным способом...

\item \emph{Вставка в дерево}: процедура \texttt{Insert}. Процедура будет принимать дерево и новую вершину, а возвращать изменённое дерево.
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def Insert(T, node)
	L, R = Split(T, node.k)
	return Merge(Merge(L, node), R)
\end{minted}
Заметим, что мы используем тут целых 2 слияния и 1 сплит. Можно чуть лучше. Если у \texttt{node} приоритет выше, чем у \texttt{T}, то достаточно посплитить \texttt{T}, а потом присоединить результат к \texttt{node}. Если же приоритет меньше, то мы можем спуститься по \texttt{T} как по дереву поиска до той вершины, где её приоритет будет выше.
\begin{minted}[linenos, fontsize=\footnotesize]{python}
def Insert(T, node):
    if node.p > T.p:
        node.l, node.r = Split(T, node.k)
        return node
    if node.k < T.k:
        T.l = Insert(T.l, node) 
    else:
        T.r = Insert(T.r, node)
    return T # root of result tree is T
\end{minted}

\item \emph{Удаление из дерева}: процедура \texttt{Remove}. Принимаем дерево и ключ, а возвращаем дерево, их которого вершинка с ключиком была удалена.
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def Remove(T, k)
	L, R = Split(T, k)
	D, R = Split(R, k)
	return Merge(L, R)
\end{minted}
Тут делается 2 сплита и 1 слияние, но можно лучше. Ясно, что если \texttt{T.k == k}, то нам просто нужно вернуть слитых левого и правого сына \texttt{T}. Этим и воспользуемся:
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def Remove(T, k)
    if T.k == k:
        return Merge(T.l, T.r)
    if k < T.k:
        T.l = Remove(T.l, k)
    else:
        T.r = Remove(T.r, k)
    return T
\end{minted}

\end{itemize}

\begin{note} 
Легко видеть, что время работы операций \texttt{Split} и \texttt{Merge} --- $\bigO{h}$, где $h$ --- высота дерева. А значит аналогично и время работы вставки и удаления.
\end{note}

\subsection{Дуча (Treap, Дерамида)}

\begin{defn}
Дуча --- это декартово дерево, приоритеты в котором случайные и равномерно распределённые.
\end{defn}
\begin{lem}
Математическое ожидание высоты дучи равно $\bigO{\log{n}}$, где $n$ --- число вершин в ней.
\end{lem}
\begin{proof}
Обратимся к рекурсивному (тот, что за $\bigO{n^2}$) алгоритму построения декартова дерева. Заметим, что в силу случайности приоритетов мы равновероятно выбираем любой ключ при построении дерева, относительно которого все остальные ключи будут разбиты по группам $<$ или $\geq$. Это в точности та же процедура, которая происходит при применении быстрой сортировки. И дерево рекурсии для этих задач идентичны. Умеем доказывать, что дерево рекурсии \texttt{QuickSort} будет иметь высоту, в среднем (из-за случайности процесса, а не по входам), логарифмическую! 
\end{proof}
Выводы: таким образом \texttt{Treap} -- это дерево поиска, операции на котором будут работать за $\bigO{\log{n}}$.

\section{Задачи \texttt{RMQ} и \texttt{LCA}}

\begin{defn}
Дан массив $A$. Задача \texttt{\textbf{RMQ}} или \texttt{Range Minimum Query} заключается в том, что по запросу $[l, r]$, $l \leq r$, $l, r \in \mathbb{N}$ нужно найти $\min_{i \in [l, r]}{A[i]}$.
\end{defn}
\begin{defn}
Пусть дано дерево $T$. Задача \texttt{\textbf{LCA}} или \texttt{Least Common Ancestor} заключается в том, что по запросу $(v, u)$, где $v$ и $u$ есть вершины дерева $T$, нужно найти вершину $p \in T$ такую, что $p$ --- общий предок $v$ и $u$ и при этом он самый близкий к ним, т.е. нету вершины $p'$, которая общий предок $v$ и $u$ и при этом её глубина больше глубины $p$.  
\end{defn}
Задачи \texttt{RMQ} и \texttt{LCA} можно решать в разных условиях: когда исходный массив или дерево могут меняться (динамическая задача) или когда они даны и неизменны (статическая задача).
\begin{note}
Заранее скажем, что существует круговорот \texttt{RMQ} и \texttt{LCA} в природе, а именно: \texttt{RMQ} сводится к \texttt{LCA}, а \texttt{LCA} сводится к \texttt{RMQ$\pm 1$}, что есть частный случай задачи \texttt{RMQ}.
\end{note}
\begin{note}
В лоб задачи \texttt{LCA} и \texttt{RMQ} решаются за $\bigO{n}$ (ответ на один запрос, на дерево никаких ограничений в духе сбалансированности нет). Но мы научимся (возможно, описано тут некоторое не будет) решать быстрее:
\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
            & Дерево отрезков & Полный предподсчёт & Sparse Table \\
\hline
предподсчёт & $\bigO{n}$        & $\bigO{n^2}$         & $\bigO{n\log{n}}$ \\
\hline
запрос      & $\bigO{\log{n}}$  & $\bigO{1}$ & $\bigO{1}$ \\
\hline
динамически?  & \color{OliveGreen}{да} & \color{red}{нет} & \color{red}{нет} \\
\hline
\end{tabular}
\caption{Задача \texttt{RMQ}}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
            & Двоичные подъёмы & Полный предподсчёт & \texttt{Ladder decomposition} \\
\hline
предподсчёт & $\bigO{n\log{n}}$        & $\bigO{n^2}$ &  $\bigO{n\log{n}}$ \\
\hline
запрос      & $\bigO{\log{n}}$  & $\bigO{1}$ & $\bigO{1}$\\
\hline
динамически?  & \color{red}{нет} & \color{red}{нет} & \color{red}{нет}\\
\hline
\end{tabular}
\caption{Задача \texttt{LCA}, если решать именно как \texttt{LCA} :) Под \texttt{Ladder Decomposition} понимается разложения дерева на <<длиннейшие>> пути, а потом наворачивание поверх этого двоичных подъёмов (самый обычных), в этом документе про это точно не будет. }
\end{table}
Так же мы научимся сводить \texttt{RMQ} к \texttt{LCA} и наоборот за линейное $\bigO{n}$ время, так что методы решения одной задачи можно использовать для решения другой.
\end{note}
\subsection{Динамический \texttt{RMQ} и \texttt{RSQ}. Дерево отрезков}
Пускай у нас есть ассоциативная операция $\circ$, например $\min$ или $+$. Заметим тогда, что при вычислении $A[l] \circ A[l+1] \circ A[l+2] \circ \ldots \circ A[r]$ мы можем в силу ассоциативности расставлять скобки как хотим. Построим такую структуру данных, которая нам будет разбивать отрезок запроса на некоторое число \emph{непересекающихся} подотрезков, на которых ответ уже посчитан. Тогда, в силу ассоциативности, мы можем получить ответ на всё отрезке. Далее в качестве операции будем рассматривать, например, $+$.
\begin{defn}
\emph{Дерево отрезков} --- это такое бинарное дерево, что его корень ассоциирован со всем массивом $A[1..n]$, а дети вершины, которой соответствует кусок массива $A[l..r]$, ассоциированы с кусками массива $A[l..m]$ и $A[m+1..r]$, где $m = \frac{l + r}{2}$. См. рисунок~\ref{fig:segtree}
\end{defn}
В каждой вершине такого дерева мы можем хранить ответ на запрос на отрезке, которому эта вершина соответствует.

\subsubsection{Построение дерева отрезков}
Построить дерево можно за линию. Считаем, что длина исходного массива $n = 2^k$. Всего вершин в дереве $2n$, т.к. это полное бинарное дерево на $n$ листьях (можешь доказать это через лемму о рукопожатиях, например). Само дерево можно хранить в массиве \texttt{tree[2*n]} так, что \texttt{tree[1]} --- это корень дерева отрезков, который соответствует всему массиву. А дети \texttt{i}-ой вершины --- это вершины \texttt{tree[2*i]} и \texttt{tree[2*i+1]}. Т.е. отец вершины \texttt{tree[j]} --- это \texttt{tree[j / 2]}.
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def build(arr):
    tree = [0] * (2 * len(arr) + 1)
    tree[len(arr) + 1:] = list(arr)
    for i in reversed(range(1, len(arr)+1)):
        tree[i] = tree[2 * i] + tree[2 * i + 1]
    return tree
\end{minted}
Ясно, что строим за $\bigO{n}$.
\begin{figure}[ht!]
\centering
\input{segment_tree.pdf_tex}
\caption{Дерево отрезков для данного массива $A$. Если размер $A$ не степень двойки, то добьём до степени двойки нейтральными элементами относительно той операции, которую хотим считать на отрезках массива.}
\label{fig:segtree}
\end{figure}

\subsection{Ответ на запрос и оценка на время работы}
Отвечать на запрос о сумме (минимуме, ...) на отрезке будем декомпозируя искомый отрезок на куски, которые представляют из себя вершины дерева отрезков.
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def sum(v, l, r, ql, qr):
    """
    v - индекс вершины дерева отрезков в массиве tree
    l, r - границы отрезка, за которые вершина v отвечает
    ql, qr - текущие границы запроса, ql >= l и qr <= r
    """
    if ql > qr:
        return 0
    if l == ql and r == qr:
        return tree[v]
    m = (l + r) / 2
    return sum(2*v, l, m, ql, min(m, qr)) + sum(2*v+1, m+1, r, max(m+1, ql), qr)
\end{minted}
Нужно понять, как быстро это работает?
\begin{lem}
Вызов \mintinline{python}{sum(1, 0, n-1, l, r)} отрабатывает за $\bigO{\log{n}}$.
\end{lem} 
\begin{proof}
Заметим следующую вещь. Всего может быть 2 случая:
\begin{enumerate}[label=\arabic*.]
\item Если какая-то граница отрезка \texttt{[l, r]} совпала с какой-то границой отрезка \texttt{[0, n-1]}.
\begin{figure}[ht!]
\centering
\def\svgwidth{0.4\columnwidth} 
\input{rec_seg_1.pdf_tex}
\caption{Хотя бы одна из границ совпала.}
\label{fig:rec_seg_1}
\end{figure}
Видим тогда, что у нас один из ветвей рекурсии сразу же остановится и останется ещё всего лишь одна. Т.е. можно считать, что число ветвей рекурсии не увеличивается.
\item \label{segfast:case:1} Если у нас ни одна из границ не совпала и при этом отрезок \texttt{[l,r]} не пересекает середину отрезка, соответствующего вершине, то тогда у нас опять же число ветвей рекурсии на этом шаге не увеличивается.
\item Если же ни одна из границ не совпала, а отрезок \texttt{[l,r]} пересекает середину, то ситуация получается такой:
\begin{figure}[ht!]
\centering
\def\svgwidth{0.4\columnwidth} 
\input{rec_seg_2.pdf_tex}
\caption{Отрезок \texttt{[l, r]} пересекает середину отрезка вершины}
\label{fig:rec_seg_2}
\end{figure}
Видим тогда, что у нас увеличилось число ветвей рекурсии. Их стало две. Но заметим тогда, что обе эти ветви рекурсии удовлетворяют случаю~\ref{segfast:case:1}. А значит, что ни одна из порождённых ветвей рекурсии не расплодится (больше чем на один шаг).
\end{enumerate}
Получили таким образом, что максимальное число ветвей рекурсии равно $4$ (это видно на рисунке~\ref{fig:rec_seg_2}), но это значит, что время работы ответа на запрос пропорционально высоте дерева отрезков, а высота этого дерева, очевидно, $\log{n}$. Вот и получили $\bigO{\log{n}}$ на запрос.
\end{proof}
\begin{note}
Из леммы так же следует, что мы можем любой отрезок разбить на $\bigO{\log{n}}$ отрезков из дерева (канонических отрезков). Иначе мы бы не могли получить такой асимптотики.
\end{note}
\subsubsection{Обновление элемента массива}
Тут всё просто. Каждый элемент массива покрывается ровно $\log{n}$ каноническими отрезками. Это, к слову, хорошо видно из рисунка~\ref{fig:segtree}. Таким образом нам нужно просто обновить значение на всех этих отрезках. Это делается либо проходом сверху вниз, либо снизу вверх (тут нам помогает наш способ хранения дерева через массив и удобная адресация к родителю вершины).
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def update(v, l, r, idx, val):
    if l == r and l == idx:
        tree[l] = val
    else:
        m = (l + r) / 2
        if idx <= m:
            update(2*v, l, m, idx, val)
        else:
            update(2*v+1, m+1, r, idx, val)
        tree[v] = tree[2*v] + tree[2*v+1] # or min(tree[2*v], tree[2*v+1])
\end{minted}
Тут совсем ясно, что мы тратим ровно высоту времени, т.е. $\bigO{\log{n}}$
\begin{note}
Быстрее делать обе операции (запрос и изменение) нельзя. Если бы было можно, то и массив бы мы отсортировали быстрее, чем за $\bigO{n\log{n}}$ (достаём минимум, заменяем элемент по его индексу на $\infty$), а это теоретически невозможно.
\end{note}

\subsection{Статический \texttt{RMQ}. Sparse table}
Пускай у нас теперь исходный массив фиксирован и не меняется. Нужно лишь уметь отвечать на запросы минимум на отрезке \texttt{[l, r]}.
\begin{enumerate}[label=\arabic*.]
\item Полный предподсчёт. Можно просто взять и для всех пар подсчитать ответ за $\bigO{n^2}$, после чего отвечать на каждый запрос за $\bigO{1}$. 
\item \texttt{\textbf{Sparse Table}} или Разреженная таблица. В данной ситуации нам важно, что операция, которую мы хотим считать на массиве, \emph{идемпотентна}. Заметим такую вещь: если исходный запрос $[l, r]$ мы разобъём на 2 \emph{пересекающихся} подотрезка $[l, l + 2^k]$ и $[r - 2^k, r]$, то $\min_{[l, r]} = \min(\min_{[l, l + 2^k]}, \min_{[r - 2^k, r]})$. Тут $2^k$ максимальное такое, что $r - l + 1 \geq 2^k$. Суть в том, что такие 2 отрезка гарантированно покроют весь отрезок $[l, r]$. Суть метода \texttt{Sparse Table} в том, что мы предподсчитываем ответы на запросы на всех отрезках, длина которых --- степень двойки. Вплоть до длины $\log{n}$. Таким образом предподсчёт у нас займёт $\bigO{n\log{n}}$. Идемпотентность нам нужна, т.к. отрезки, на которые мы разбиваем запрос, могут пересекаться.
\item Для операций, у которых есть обратная операция, мы можем предподсчитать за $\bigO{n}$ результаты на префиксах массива, а потом отвечать за $\bigO{1}$.
\end{enumerate}

\subsection{\texttt{LCA}. Эйлеровы обходы. Сведение к \texttt{RMQ} и наоборот}
\subsubsection{Сведение \texttt{RMQ} к \texttt{LCA}}
Пусть у нас есть массив $A[1..n]$. Рассмотрим набор пар $(A[i], i)$ и построим по нему декартово дерево, где $i$ (индексы в массиве) будут ключами, а приоритетами будут $A[i]$. Ясно, что т.к. полученное дерево будет деревом поиска по индексам и кучей по элементам массива, то наибольший общий предок вершины $(A[l], l)$ и вершины $(A[r], r)$ --- это такая вершина $(A[i], i)$, что $l \leq i \leq r$ и $A[i] \leq A[l], A[r]$. Это ясно по построению и свойствам дерева.

\subsubsection{Эйлеров обход}
Эйлеровы обходы бывают разные. Пускай у нас есть дерево:
\begin{figure}[ht!]
\centering
\begin{tikzpicture}[scale=1, baseline=(current bounding box.center)]
	\tikzstyle{every node}=[font=\scriptsize]
\begin{scope}[VertexStyle/.append style = {minimum size = 15pt}]
    \Vertex[Math, x=0, y=0]{1}
    \Vertex[Math, x=0.5, y=-1]{3}
    \Vertex[Math, x=0, y=-2]{5}
    \Vertex[Math, x=-1, y=-2]{4}
    \Vertex[Math, x=-0.5, y=-1]{2}
\end{scope}
\foreach \x/\y in {1/2, 1/3, 2/4, 2/5} {\Edge(\x)(\y)}
\end{tikzpicture}
\end{figure}
Эйлеров обход --- это своеобразный протокол обхода в глубину данного дерева.
Эйлеровы обходы бывают следующие:
\begin{enumerate}
	\item \emph{Обход по рёбрам}. Обходим дерево начиная с корня по рёбрам слева направо. Проходя по ребру добавляем его к обходу.
	\[ (1,2),\ (2,4),\ (4,2),\ (2,5),\ (5,2),\ (2,1),\ (1,3),\, (3,1) \]
	\item \emph{Обход по вершинам с повторениями.}
	\[ 1, 2, 4, 2, 5, 2, 1, 3, 1 \]
	\item \emph{Обход по вершинам}, где вершина добавляется в обход при входе в её поддерево и при выходе.
	\[ 1, 2, 4, 4, 5, 5, 2, 3, 3, 1\]
\end{enumerate}
Будем эйлеровым обходом называть обход по вершинам с повторениями. Заметим следующую вещь. Пронумеруем все вершины в порядке обхода \emph{в ширину}, т.е. чтобы у вершины с меньшей глубиной всегда был больший номер. Теперь пусть мы хотим найти \texttt{LCA} вершин $a$ и $b$. Тогда посмотрим на индекс $i$ первого вхождения $a$ в эйлеровом обходе и индекс $r$ первого вхождения $j$ в эйлеровом обходе. Не умаляя общности пускай $i < j$. Что может находится между позициями $i$ и $j$ в эйлеровом обходе? В силу устройства эйлерова обхода, если $b$ не находится в поддереве $a$, то между $i$ и $j$ может лежать единственный общий предок $a$ и $b$ и причём он будет наименьшим (\texttt{least common ancestor}) и он будет иметь наименьший номер. Если же $b$ лежит в поддереве $a$, то $a$ и будет ответом на запрос. Заметим, что в силу нумерации получается, что для ответа на запрос нам нужно найти минимум на отрезке $[i, j]$ в эйлеровом обходе. Таким образом будем хранить массив $first[1..n]$ первых вхождений каждой вершины. Если потребуется найти $lca(a,b)$, то нужно спрашивать $rmq(first[a], first[b])$. Свели.
\begin{note}
Какова длина эйлерова обхода? Если смотреть на то, как мы ходим по рёбрам, то можно заметить, что в обход мы добавляем конец каждого ребра (когда идём вних) и начало каждого ребра (когда поднимаемся наверх), плюс ещё корень, который появляется вначале, потому что с него начинаем (очевидно). Итого $2(n-1)+1 = 2n-1$. А значит сведение работает за $\bigO{n}$.
\end{note}
\begin{note}
Существует алгоритм решения \texttt{RMQ$\pm 1$} за $\bigO{n}$ предподсчёт и $\bigO{1}$ на запрос. А т.к. мы всё ко всему умеем сводить, то для $LCA$ и $RMQ$ в общем случае тоже можно получить такую асимптотику.
\end{note}

\section{Хеширование 1}
Довольно часто для решения тех или иных задач нам нужно хранить объекты в множестве, возможно с дополнительной информацией (в таком случае это будет уже словарь или \texttt{map}). Абстрактная структура данных \textbf{<<множество>>} должно уметь 3 вещи:
\begin{itemize}
    \item \texttt{Insert} или вставка объекта
    \item \texttt{Delete} или удаление объекта
    \item \texttt{Find} или поиск объекта
\end{itemize}
Заметим, что если над объектами есть операция сравнения, а точнее линейный порядок, то можно в качестве структуры данных заиспользовать сбалансированное дерево поиска и получить $\bigO{\log{n}}$ время работы на всех трёх операциях.
\begin{note}
Сейчас вас ждёт выжимка из Кормена (почти).
\end{note}
\emph{Обозначаем} $U$ --- множество объектов, которые хотим хранить в множестве.

\subsection{Прямая адресация. Хеширование. Коллизии}
\begin{defn}
Пускай у нас множество $U$ представляет из себя подмножество целых положительных чисел. Тогда \emph{таблица с прямой адресацией} --- это такой массив $T$ размера $|U|$, что $T[k] = NIL$, если элемента $k$ в <<множестве>> нет, а иначе есть (если что-то в этой ячейке лежит, это могут быть любые сопутствующие данные).
\end{defn}
Операции с таблицей с прямой адресацией все работают за $\bigO{1}$. Минус --- размер множества $U$ может быть велик, что плохо. На помощь приходит хеширование.
\begin{defn}
\emph{Хеш-функция} --- это функция из множества объектов $U$ в множество $\xbrace{0, 1, \ldots, m - 1}$:
\[
    h: U \to \xbrace{0, 1, \ldots, m - 1}
\]
\end{defn}
Будем теперь хранить таблицу размера $m$ и для вычисления индекса объекта $x$ в таблице $T$ будем дёргать функцию $h(x)$. Теперь для хранения таблицы нам нужно $\bigO{m}$ памяти. Такая таблица уже, внезапно, называется \textbf{хеш-таблицей}! Но есть проблемы, ведь если $|U| > m$, то по принципу Дирихле (sic!) хотя бы для двух объектов хеши совпадут.
\begin{defn}
\emph{Коллизия} --- это когда $x, y \in U$, $x \neq y$, но $h(x) = h(y)$
\end{defn}
\subsection{Гипотеза равномерного хеширования}
Прежде чем идти рассуждать про разрешения коллизий введём гипотезу.
\begin{defn}
Пускай есть хеш-функция $h: U \to \xbrace{0, 1, \ldots, m-1}$. Тогда \emph{гипотеза равномерного хеширования} гласит, что для любого объекта $x \in U$ вероятность того, что $h(x)$ равна конкретному значению $i \in  \xbrace{0, 1, \ldots, m-1}$, равна $\frac{1}{m}$. 
\[
    \forall i \in \xbrace{0, \ldots, m}\ (P\{h(x) = i\} = \frac{1}{m})
\]
Т.е. хеш-функция равномерно разбрасывает все объекты по индексам.
\end{defn}
\subsection{Разрешение коллизий цепочками}
Будем в каждой ячейке $T[i]$ хеш-таблицы хранить связный список, тогда операции немного изменятся:
\begin{itemize}
    \item \texttt{Find(x)}. Вычисляем $h(x)$ и линейным поиском ищем $x$ в списке $T[h(x)]$ 
    \item \texttt{Insert(x)}. Делаем \texttt{Find(x)}, если он ничего не нашёл, то добавляем в конец списка.
    \item \texttt{Delete(x)}. Аналогично! За линию от длины цепочки удаляем из неё элемент.
\end{itemize}
Давайте теперь анализировать... Пускай в хеш-табличке у нас лежит $n$ элементов, а при этом различных индексов (т.е. цепочек) $m$.
\begin{defn}
\emph{Коэффициент заполнения} $\alpha$ --- это средняя длина цепочки в хеш-таблице.
\[
    \alpha = \frac{n}{m}
\]
\end{defn}
Заметим, что все операции у нас работают столько же, сколько и \texttt{Find(x)} (почти). Достаточно его проанализировать, а потом уж если что замечания сделаем.
\begin{thm}
Пускай \textbf{верна гипотеза равномерного хеширования}. Тогда \emph{среднее} время неуспешного поиска \texttt{Find(x)} равно $\bigO{1 + \alpha}$
\end{thm}
\begin{proof}
Ясно, что среднее время неуспешного поиска в пропорционально средней длине цепочки плюс ещё время на вычисление хеша, а значит равно:
\[
    E[\text{время неуспешного поиска}] = 1 + E[\text{длина цепочки}] = 1 + \alpha
\]
\end{proof}
\begin{note}
Заметим, что среднее время нужное на \texttt{Insert(x)}, если $x$ ещё не был вставлен в таблицу, пропорционально $1 + \alpha$, т.е. равно $\bigO{1 + \alpha}$ (единичка на саму вставку!).
\end{note}
\begin{thm}
Пускай \textbf{верна гипотеза равномерного хеширования}. Тогда \emph{среднее} время \emph{успешного} поиска \texttt{Find(x)} равно $\bigO{1 + \alpha}$
\end{thm}
\begin{proof}
Заметим следующее: время успешного поиска равно в точности времени неуспешного поиска при вставке, т.к. вставляем мы в конец. Вставить элемент мы могла на одной из $n$ вставок, чтобы найти среднее время успешного поиска нужно усреднять по вставкам!
\begin{align*}
    E[\text{время успешного поиска}] &= \frac{1}{n}\sum_{i = 1}^{n}{E[\text{неуд. поиск при } i-1 \text{ размере таблицы}]} = \\
    & = \frac{1}{n}\sum_{i = 1}^{n}{1 + \frac{i- 1}{m}} = \\
    & = 1 + \sum_{i = 1}^{n}{\frac{i-1}{m}} = [\text{$i - 1$ т.к. вставка номер $i$}\\
    & = 1 + \frac{1}{nm}\frac{(n - 1)n}{2} = 1 + \frac{n - 1}{2m} = 1 + \frac{n}{2m} - \frac{1}{2m} \\
    & = 1 + \frac{\alpha}{2} - \frac{\alpha}{2n} = \bigO{1 + \alpha}
\end{align*}
\end{proof}
\begin{note}
Видно, что в среднем, при верности гипотезы равномерного хеширования, операции с хеш-таблицей работают за $\bigO{1 + \alpha}$.
\end{note}
\subsection{Разрешение коллизий пробами}
Разрешать коллизии можно по-другому, без затрат на хранение указателей на списки, как это было при использовании цепочек. Для этого расширим понятие хеш-функции.
\begin{defn}
Хеш-функция: $h: U \times \xbrace{0, 1, \ldots, m - 1} \to \xbrace{0, 1, \ldots, m - 1}$. Тут второй аргумент хеш-функции --- это номер \emph{пробы}.
\end{defn}
При выполнении операции вставки мы обращаемся в ячейку по хешу $h(x, 0)$, если эта ячейка занята, то смотрим на $h(x, 1)$ и так далее.
\begin{note}
Ясно, что по-хорошему нужно, чтобы пробы охватывали всю хеш-таблицу $T$ из $m$ элементов, т.е. чтобы $\xangle{h(x, 0), h(x, 1), \ldots, h(x, m - 1)}$ было перестановкой множества $\xbrace{0, 1, \ldots, m-1}$, для любого $x$.
\end{note}
\begin{note}
Гипотеза равномерного хеширования в данном случае тоже немного меняется, а именно нам хочется, чтобы расширенная хеш-функция $h(x,i)$ выдавала перестановки множества $[0,m)$ равновероятно (перестановок всего $m!$). И вообще было бы круто, чтобы она выдавала все перестановки.
\end{note}
\subsubsection{Линейные пробы}
Пусть у нас есть хеш-функция $h: U \to \xbrace{0, \ldots, m-1}$. Тогда в методе линейных проб используется расширенная хеш функция:
\[
    h(x, i) = (h(x) + i) \bmod m
\]
Ясно как это работает. Просто последовательно исследуем табличку в случае занятой ячейки.
\begin{note}
Проблема --- кластеризация. Пускай есть какая-то длинная цепочка подряд идущих занятых ячеек, тогда в силу вероятности, она имеет тенденцию увеличиваться ещё сильнее, что в конечном итоге приводит к замедлению работы.
\end{note}
Метод линейных проб позволяет получить лишь $m$ перестановок (циклических сдвигов, на самом деле) множества $\xbrace{0, \ldots, m-1}$.
\subsubsection{Квадратичные пробы}
Аналогично, пусть есть функция $h: U \to \xbrace{0, \ldots, m-1}$, тогда в данном методе используется:
\[
    h(x, i) = (h(x) + c_1i + c_2i^2) \bmod m
\]
Тут нужно аккуратно подобрать $c_1, c_2$, чтобы пробы охватили всю таблицу. В данном случае \emph{кластеризация} будет меньшей, она будет вторичной.
Тут опять мы покроем лишь $m$ перестановок, но это уже будут не циклические сдвиги, а что-то чуть более хитрое.
\subsubsection{Двойное хеширование}
Тут у нас пусть есть две функции: $h_1(x)$ и $h_2(x)$. Обе как-то хешируют объекты множества $U$.
Тогда двойное хеширование использует следующую расширенную функцию:
\[
    h(x, i) = (h_1(x) + ih_2(x)) \bmod m
\]
Чтобы добиться того, чтобы пробы $h(x, i)$ составляли перестановку множества $\xbrace{0, \ldots m-1}$ нужно заметить:
\[
    |\xbrace{h_1(x), h_1(x) + h_2(x), \ldots, h_1(x) + (m-1)h_2(x)}| = |\xbrace{0, h_2(x), \ldots, (m-1)h_2(x)}|
\]
Т.е. нас интересует, чтобы каждое уравнение $i \cdot h_2(x) = j \bmod m$ (для каждого $j$) имело единственное решение. Это достигается только тогда, когда $gcd(h_2(x), m) = 1$. 
\begin{note}
Математику можно подробнее посмотреть в главе 31 Кормена.
\end{note}
\subsubsection{Оценки на время работы}
\begin{note}
Тут мы оцениваем время работы для некоего сферического в вакууме метода проб. У него всё хорошо и он умеет генерировать \emph{все} перестановки равновероятно.
\end{note}
\begin{note}
\label{note:probe_hyp}
Будем считать что верна гипотеза равномерного хеширования, это будет значить при добавлении элемента, вероятность того, что мы просмотрим $i$-ую ячейку (по количеству), при условии что уже просмотрели $i - 1$ ячейку равна:
\[
    \frac{\text{число ещё не просмотренных заполненных элементов}}{\text{число всех ещё не просмотренных элементов}} = \frac{n - i + 1}{m - i + 1}
\]
\end{note}
Считаем, что у нас везде $n < m$, т.е. таблица не забита до отказа.
\begin{thm}
При гипотезе равномерного хеширования время работы неудачного поиска равно $\frac{1}{1 - \alpha}$.
\end{thm}
\begin{proof}
$X$ --- случайная величина равна \emph{числу проб} проделанных до того, как наткнулись на пустую ячейку. $A_i$ --- событие, что мы сделали $i$-ую пробу. Тогда посчитаем такую вероятность:
\begin{align*}
    P\xbrace{X \geq i} &= P\xbrace{A_1 \cdot A_2 \cdot \ldots \cdot A_{i - 1}} = \\
                       &= P\xbrace{A_{i-1} | A_{1} \cdot \ldots \cdot A_{i-2}} P\xbrace{ A_{1} \cdot \ldots \cdot A_{i-2}} = \\
                       &\vdots \\
                       &= P\xbrace{A_{1}}P\xbrace{A_2|A_1}P\xbrace{A_3|A_1\cdot A_2}\dots P\xbrace{A_{i-1}|A_{1} \cdot \ldots \cdot A_{i-2}}=\\
                       &\text{применяем замечание~\ref{note:probe_hyp}}\\
                       &=\frac{n}{m}\cdot\frac{n-1}{m-1}\dots\frac{n-i+2}{m-i+2}=\\
                       &\text{легко видеть, что $\frac{n-i}{m-i} \leq \frac{n}{m}$ если $n < m$}\\
                       &\leq (\frac{n}{m})^{i - 1} = \alpha^{i - 1}
\end{align*}
Нас интересует математическое ожидание числа проб, т.е. $X$:
\begin{align*}
    E[X] = \sum_{i = 0}^{\infty}{iP\xbrace{X = i}} = \sum_{i = 1}^{\infty}{i(P\xbrace{X \geq i} - P\xbrace{X \geq i + 1})} = \sum_{i = 1}^{\infty}{P\xbrace{X \geq i}} = \sum_{i=1}^{\infty}{\alpha^{i - 1}} = \frac{1}{1 - \alpha}
\end{align*}
Вот собственно и всё, ибо это и есть среднее время при неудачном поиске --- среднее число неуспешных проб (проб по занятым ячейкам)!
\end{proof}
Это же время равно времени вставки в таблицу, если элемента там ещё нет.
\begin{thm}
Среднее число исследований при успешном поиске, в предположении равномерного хеширования, равно:
\[
    \frac{1}{\alpha}\ln{\frac{1}{1 - \alpha}}
\]
\end{thm}
\begin{proof}
Среднее время при успешном поиске равно усреднённому времени вставки по всем вставкам, ибо элемент, который мы ищем, был когда-то вставлен! Т.е. среднее число исследований равно:
\begin{align*}
    E[\text{число проб при успешном поиске}] &= \frac{1}{n}\sum_{i = 1}^{n}{\frac{1}{1 - \frac{i}{m}}} = \frac{1}{n}\sum_{i = 0}^{n-1}{\frac{1}{1 - \frac{i}{m}}} = \frac{m}{n}\sum_{i = 0}^{n-1}{\frac{1}{m - i}} =\\
    &= \frac{1}{\alpha}\sum_{i = m-n+1}^{m}{\frac{1}{i}} \leq \frac{1}{\alpha}\int_{m-n}^{m}{\frac{1}{x}dx} = \frac{1}{\alpha}\ln{\frac{1}{1 - \alpha}}
\end{align*}
Воспользовались ограничением суммы интегралом, это норм.
\end{proof}
Ну вот как-то так.

\section{Хеширование 2}
Плохой чел может взять и подобрать последовательность ключей, которые все неким образом упадут в одну цепочку или выстроятся как-нибудь в линию при использовании метода проб. Чтобы как-то это пресечь можно от запуска к запуску выбирать случайную хеш-функцию. Будем об этом говорить.

\subsection{Универсальное семейство хеш-функций}
\begin{defn}
\emph{Универсальное семейство хеш-функций} $\mathcal{H}$ --- это такое множество хеш-функций $h: U \to \xbrace{0,\ldots,m}$, что для любых $x,y \in U$ и \emph{случайно выбранной} хеш-функции $h \in \mathcal{H}$ верно:
\[
    P\xbrace{h(x) = h(y)} \leq \frac{1}{m}
\]
\end{defn}

\subsection{Универсальное семейство для целочисленных ключей}
Оказывается можно легко построить универсальное семейство, если $U$ --- это целые числа.
\begin{thm}
Семейство хеш-функций:
\[
    \mathcal{H}_{p,m} = \xbrace{h(x) = ((ax + b) \bmod p) \bmod m\ |\ a \in \mathbb{Z}^*_p, b \in \mathbb{Z}^+_p}
\]
\textbf{является универсальным}. 
Тут у нас $p$ --- простое, а $\mathbb{Z}^*_p, b \in \mathbb{Z}^+_p$ --- это, соответственно, мультипликативная и аддитивная группы по модулю $p$, а в силу его простоты это просто множества $\xbrace{1,\ldots,p-1}$ и $\xbrace{0,\ldots,p-1}$.
\end{thm}
\begin{proof}
Рассматриваем два числа $x$ и $y$. В силу простоты $p$ получается, что $ax+b \bmod p$ не равно $ay+b\bmod p$ (рассматриваем разность этих тождеств). Т.е. коллизий пока не случилось, а все коллизии, которые могут случиться случаются лишь при взятии по модулю $m$. Т.е. вероятность коллизии --- это на самом деле вероятность того, что два числа $r$ и $s$ таких, что $r \neq s \bmod p$, равны по модулю $m$, т.е. $r = s \bmod m$. Как $r$ так и $s$ могут принимать $p$ возможных значений. Но т.к. $r \neq s$, то для конкретного $r$ существует всего $p-1$ возможных $s$. А число таких $s$, что будет верно $r - s = 0\bmod m$ уж всяко не больше (т.к. это число всех чисел, которые делятся на $m$ меньше $p$) $\ceil{\frac{p}{m}}-1\leq \frac{p+m-1}{m}-1=\frac{p-1}{m}$. (Тут, как я понимаю, мы имеем право закрепить $r$ и относительно него смотреть сколько $s$ таких, каких нам нужно, ибо нам изначально дано 2 каких-то числа из которых всё и выходит...). А значит, вероятность того, что произойдёт коллизия $\leq \frac{(p-1)/m}{p-1} = \frac{1}{m}$.
\end{proof}

\subsection{Универсальное семейство и разрешение коллизий цепочками}
Пускай хеш-функция $h$ выбирается из универсального семейства $\mathcal{H}$. Тогда покажем что-нибудь про время на успешный и неуспешный поиск. Помним, что $\alpha = \frac{n}{m}$ --- коэффициент загруженности хещ-таблицы.

Введём случайную величину:
\[ 
    X_{xy} = \begin{cases} 1, &h(x) = h(x)\\ 0, &h(x) \neq h(y) \end{cases}
\]
Тут $x, y$ --- объекты, положенные в хеш-таблицу $T$. Тогда следующая случайная величина --- это размер цепочки, в которой лежит объект $x$.
\[
    Y_x = \sum_{y \in T, y \neq x}{X_{xy}}
\]
Заметим, что $X_{xy}$ --- индикаторная случайная величина, а значит $E[X_{xy}] = P\xbrace{h(x)=h(y)} \leq \frac{1}{m}$ в силу того, что $h$ из универсального семейства.
\begin{thm}
$h \in \mathcal{H}$. Тогда математическое ожидание времени, необходимого на:
\begin{enumerate}
    \item успешный поиск равно $\bigO{1 + \alpha}$
    \item неуспешный поиск равно $\bigO{\alpha}$
\end{enumerate}
\end{thm}
\begin{proof}
Математическое время на поиск у нас пропорционально математическому ожиданию размера цепочки, в которой находится $x$. Обозначим размер этой цепочки за $E[n_{h(x)}]$
\begin{enumerate}
    \item $x$ у нас в таблице есть, но $Y_x$ его не учитывает. А $|T \setminus x| = |T| - 1 = n - 1$ 
    \[
        E[n_{h(x)}] = 1 + E[Y_x] = 1 + \frac{n - 1}{m} = 1 + \alpha - \frac{1}{m} < 1 + \alpha
    \]
    \item Самого $x$ в таблице нет, т.е. $|T \setminus x| = |T| = n$, а значит:
    \[
        E[n_{h(x)}] = E[Y_x] = \sum_{y\in T}{E[X_{xy}]} \leq \sum_{y \int T}{\frac{1}{m}} = \frac{n}{m} = \alpha 
    \]
\end{enumerate}
\end{proof}
Во дела! В среднем, выходит, всё хорошо.

\subsection{Идеальное хеширование}
Иногда бывает (тут Корменовский пример о таблице зарезервированном символов в компиляторе), что хеш-таблица строится один раз, а потом уже не изменяется. Тогда, оказывается, что можно сделать её такой, чтобы она работала за $\bigO{1}$ в \textbf{худшем} случае!
Как это сделать:
\begin{itemize}
    \item Идея: в каждой ячейке хеш-таблицы $T$ хранится ещё одна хеш-таблица (второго уровня). Т.е. у нас одна хеш-таблица верхнего уровня и $m$ хеш-таблиц второго уровня
    \item Хеш-функция на верхнем уровне: $h \in \mathcal{H}_{p, m}$ (т.е. из того универсального семейства, которое мы умеем строить)
    \item Сначала строим обычную хеш-таблицу с использованием функции $h$ и разрешаем коллизии цепочками. Получаем $m$ цепочек, каждая из которых размера $n_i$. А далее каждую цепочку превращаем в хеш-таблицу... 
    \item Хеш-функции на втором уровне: $h_i \in \mathcal{H}_{p, m_i}$
    \item Причём $m_i = n_i^2$ (кажется, что дорого, но в среднем это линия, докажем ниже)
\end{itemize}
Покажем, что при таком построении мы получим, что вероятность возникновения коллизий КРАЙНЕ МАЛА! (для каждой из внутренних хеш-таблиц (второго уровня))
\begin{thm}
Вероятность возникновения коллизии при хешировании $n_i$ в хеш-таблицу размера $n_i^2$, при использовании случайной хеш-функции из универсального семейства, меньше $\frac{1}{2}$
\end{thm}
\begin{proof}
Всего возможно ${n_i \choose 2}$ коллизий, каждая с вероятностью $\frac{1}{n_i^2}$ в силу универсального семейства. Тогда:
\[
    E[\text{число коллизий}] = {n_i \choose 2}\frac{1}{n_i^2} = \frac{n_i(n_i-1)}{2n_i^2} = \frac{1}{2} - \frac{1}{2n_i} < \frac{1}{2}
\]
Теперь вспомним про неравенство Маркова:
\[
    P\xbrace{X \geq t} \leq \frac{E[X]}{t}
\]
Применив его получим, что всё доказано.
\end{proof}
Таким образом нам понадобится пару раз подобрать хеш-функцию из семейства и коллизий не будет.
Теперь увидим, что много место вся эта конструкция не займёт, а именно если $m = n$ и мы сохраняем в хеш-таблицу размера $m$ $n$ ключей:
\begin{align*}
    E\xbracket{\sum{n_j^2}} &= E\xbracket{\sum{n_j + 2{n_j \choose 2}}} =\\
                            &= E\xbracket{\sum{n_j}} + 2E\xbracket{\sum{n_j \choose 2}} = \\
                            &= n + 2E[\text{общее число коллизий}] = \\
                            &< n + 2{n \choose 2}\frac{1}{m} = n + n - 1 = 2n - 1 \leq 2n 
\end{align*}
Круто, типа. В конце пользовались тем, что $n = m$.
\begin{note}
Мне не совсем понятно, зачем нам такое нужно, если при доказательстве мы опираемся на то, что $n = m$. Чем это лучше, чем просто таблица с прямой адресацией?
\end{note}

\section{Числовые алгоритмы}
\subsection{Арифметика}
Пусть у нас есть число $N$. Тогда длина этого числа --- это то, сколько бит нужно, чтобы его уместить в памяти компьютера, т.е. это $\beta = \log{N}$. Все сложности будем мерять именно исходя из того, что размер входа --- это $\beta$.
\begin{itemize}
\item Сложение. Обычное сложение в столбик: $\bigO{\beta}$.
\item Умножение. Умножение с использование сдвига (деления на $2$) (или в столбик): $\bigO{\beta^2}$:
\begin{minted}[mathescape, linenos, fontsize=\footnotesize]{python}
def mul(a, b):
    # длины a и b $\bigO{\beta}$
    if b == 0: return 0
    res = mul(a, b >> 1) << 1 # время на сдвиги $\bigO{\beta}$, длина b уменьшается на 1
    if b % 2 == 1:
        res += a
\end{minted}
Ещё есть алгоритм Карацубы за $\bigO{\beta^{\log_2{3}}}$ и быстрое преобразование Фурье за $\bigO{\beta\log{\beta}}$.
\item Деление.
\end{itemize}
\subsection{Модульная арифметика}
\subsection{Проверка чисел на простоту}
\subsection{Генерация случайных простых чисел}
\subsection{RSA}

\end{document}