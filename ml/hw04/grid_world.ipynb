{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_grid(mat, col_wid=10, additional_format=\"\", header=None):\n",
    "    if header is not None:\n",
    "        print('{{:^{}}}'.format(1 + col_wid).format(\"\"), end=\"\")\n",
    "        print(\"{{:=^{}}}\".format((1 + col_wid) * len(mat[0]) + 1).format(\" {} \".format(header)))\n",
    "    for row in range(len(mat)):\n",
    "        print('{{:^{}}}'.format(1 + col_wid).format(\"\"), end=\"\")\n",
    "        print(\"{{:.^{}}}\".format((1 + col_wid) * len(mat[row]) + 1).format(\"\"))\n",
    "        print('{{:^{}}}'.format(1 + col_wid).format(row), end=\"\")\n",
    "        for col in range(len(mat[row])):\n",
    "            print('|{{:^{}{}}}'.format(col_wid, additional_format).format(mat[row][col]), end=\"\")\n",
    "        print(\"|\")\n",
    "    print('{{:^{}}}'.format(1 + col_wid).format(\"\"), end=\"\")\n",
    "    print(\"{{:.^{}}}\".format((1 + col_wid) * len(mat[row]) + 1).format(\"\"))\n",
    "    print('{{:^{}}}'.format(1 + col_wid).format(\"\"), end=\"\")\n",
    "    for col in range(len(mat[0])):\n",
    "        print('{{:^{}}}'.format(1 + col_wid).format(col), end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Actions\n",
    "\n",
    "UP = (-1, 0)\n",
    "DOWN = (1, 0)\n",
    "RIGHT = (0, 1)\n",
    "LEFT = (0, -1)\n",
    "IMPOSSIBLE_ACTION = (42, 42)\n",
    "\n",
    "all_actions = [UP, DOWN, RIGHT, LEFT]\n",
    "act_to_name_d = {UP: 'up', DOWN: 'down', RIGHT: 'right', LEFT: 'left'}\n",
    "name_to_act_d = {'up': UP, 'down': DOWN, 'right': RIGHT, 'left': LEFT}\n",
    "act_syms_d = {UP: 'ðŸ¡¡', DOWN: 'ðŸ¡£', LEFT: 'ðŸ¡ ', RIGHT: 'ðŸ¡¢', IMPOSSIBLE_ACTION: 'âœ–'}\n",
    "\n",
    "def act_by_name(name):\n",
    "    return name_to_act_d[name]\n",
    "\n",
    "def name_by_act(act):\n",
    "    return act_to_name_d[act]\n",
    "\n",
    "def act_to_sym(act):\n",
    "    return act_syms_d[act]\n",
    "\n",
    "def print_act_names(acts):\n",
    "    print(list(map(name_by_act, acts)))\n",
    "    \n",
    "acts_array_to_sym = np.vectorize(act_to_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "    \n",
    "    def __call__(self, action):\n",
    "        return State(self.row + action[0], self.col + action[1])\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str((self.row, self.col))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.row, self.col))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.row, self.col) == (other.row, other.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self, grid):\n",
    "        \"\"\"\n",
    "        grid -- 2d numpy array of rewards\n",
    "        \"\"\"\n",
    "        self.grid = grid\n",
    "        indices = np.where(grid != np.infty)\n",
    "        self.states = [State(p[0], p[1]) for p in zip(indices[0], indices[1])]\n",
    "        self.states_set = set(self.states)\n",
    "    \n",
    "    def _is_ok_action(self, state, action):\n",
    "        return state(action) in self.states_set\n",
    "    \n",
    "    def _check_state(self, state):\n",
    "        if state not in self.states_set:\n",
    "            raise RuntimeError(\"Bad state\")\n",
    "            \n",
    "    def perform_action(self, start_state, action):\n",
    "        new_state = start_state(action)\n",
    "        self._check_state(new_state)\n",
    "        return (new_state, self.get_reward(new_state))\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        self._check_state(state)\n",
    "        return [a for a in all_actions if state(a) in self.states_set]\n",
    "                \n",
    "    def get_reward(self, state):\n",
    "        self._check_state(state)\n",
    "        return self.grid[state.row, state.col]\n",
    "    \n",
    "    def get_all_actions(self):\n",
    "        return all_actions\n",
    "    \n",
    "    def state_f_to_mat(self, stateF, none_obj=None):\n",
    "        \"\"\"\n",
    "        any dict from states to X transforms to matrix\n",
    "        \"\"\"\n",
    "        mat = np.empty(self.grid.shape, dtype=object)\n",
    "        mat.fill(none_obj)\n",
    "        for state in stateF.keys():\n",
    "            mat[state.row, state.col] = stateF[state]\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ====================== GRID WORLD ======================\n",
      "           ........................................................\n",
      "     0     |   0.0    |   0.0    |  -100.0  |   0.0    |  100.0   |\n",
      "           ........................................................\n",
      "     1     |   0.0    |   0.0    |   0.0    |   0.0    |   inf    |\n",
      "           ........................................................\n",
      "     2     |   0.0    |   0.0    |   inf    |   0.0    |   0.0    |\n",
      "           ........................................................\n",
      "     3     |   0.0    |   0.0    |   0.0    |  -100.0  |   0.0    |\n",
      "           ........................................................\n",
      "     4     |   0.0    |   0.0    |   0.0    |   0.0    |   0.0    |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "grid = np.array([\n",
    "        [0, 0, -100, 0, 100],\n",
    "        [0, 0, 0, 0, np.infty],\n",
    "        [0, 0, np.infty, 0, 0],\n",
    "        [0, 0, 0, -100, 0],\n",
    "        [0, 0, 0, 0, 0]\n",
    "])\n",
    "print_grid(grid, col_wid=10, header=\"GRID WORLD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WORLD = World(grid=grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down', 'right']\n"
     ]
    }
   ],
   "source": [
    "print_act_names(WORLD.get_possible_actions(State(0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-value function for random walk policy\n",
    "\n",
    "Bellman equation:\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a \\in A}{\\pi(a | s) \\left( R_{s}^{a} + \\gamma \\sum_{s' \\in S}{P_{ss'}^a v_{\\pi}(s')} \\right)}\n",
    "$$\n",
    "\n",
    "For our grid world we have: $P_{ss'}^a = 0$ in case $s + a \\neq s'$ and $=1$ else. Also all rewards expectations $R_{s}^a$ are: $R_s^a = reward(s + a)$\n",
    "\n",
    "And because of random walk policy we have $\\pi(a | s) = \\frac{1}{|possibleActions(s)|}$\n",
    "\n",
    "So finally we have:\n",
    "\n",
    "$$\n",
    "v(s) = \\sum_{a \\in possibleActions(s)}{\\frac{1}{|possibleActions(s)|} \\cdot \\left(reward(s + a) + \\gamma \\cdot v(s+a)\\right) }\n",
    "$$\n",
    "\n",
    "This may be solved as system of linear equations (for every state $s$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution with SOLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_index = dict(zip(WORLD.states, range(len(WORLD.states))))\n",
    "reverse_index = dict([(i, s) for (s, i) in state_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_system():\n",
    "    system_mat = np.zeros(shape=(len(WORLD.states), len(WORLD.states)))\n",
    "    free_vec = np.zeros(shape=(len(WORLD.states), 1))\n",
    "    \n",
    "    for (state, idx) in state_index.items():\n",
    "        possible_actions = WORLD.get_possible_actions(state)\n",
    "        prob = 1.0 / len(possible_actions)\n",
    "        system_mat[idx][idx] = 1\n",
    "        for pa in possible_actions:\n",
    "            state_next = state(pa)\n",
    "            j = state_index[state_next]\n",
    "            free_vec[idx] += prob * WORLD.get_reward(state_next)\n",
    "            system_mat[idx][j] = -prob * gamma\n",
    "    return (system_mat, free_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A, b = build_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solution = np.linalg.solve(A, b)\n",
    "st_val_mat = np.zeros(shape=WORLD.grid.shape)\n",
    "for i in range(len(solution)):\n",
    "    state = reverse_index[i]\n",
    "    st_val_mat[state.row, state.col] = solution[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ====================== GRID WORLD ======================\n",
      "           ........................................................\n",
      "     0     |   0.0    |   0.0    |  -100.0  |   0.0    |  100.0   |\n",
      "           ........................................................\n",
      "     1     |   0.0    |   0.0    |   0.0    |   0.0    |   inf    |\n",
      "           ........................................................\n",
      "     2     |   0.0    |   0.0    |   inf    |   0.0    |   0.0    |\n",
      "           ........................................................\n",
      "     3     |   0.0    |   0.0    |   0.0    |  -100.0  |   0.0    |\n",
      "           ........................................................\n",
      "     4     |   0.0    |   0.0    |   0.0    |   0.0    |   0.0    |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "print_grid(WORLD.grid, header=\"GRID WORLD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ================= STATE VALUE FUNCTION =================\n",
      "           ........................................................\n",
      "     0     |  -73.22  | -103.66  |  -87.00  |  -75.57  |  -68.01  |\n",
      "           ........................................................\n",
      "     1     |  -59.04  |  -74.21  | -110.76  |  -96.88  |   0.00   |\n",
      "           ........................................................\n",
      "     2     |  -49.38  |  -56.35  |   0.00   | -136.62  | -126.97  |\n",
      "           ........................................................\n",
      "     3     |  -49.22  |  -64.25  | -117.16  | -120.42  | -145.54  |\n",
      "           ........................................................\n",
      "     4     |  -50.42  |  -62.83  |  -94.76  | -135.88  | -126.64  |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "print_grid(st_val_mat, additional_format=\".2f\", header=\"STATE VALUE FUNCTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# state value function\n",
    "svf = np.zeros(shape=grid.shape)\n",
    "eps = 1e-3\n",
    "while True:\n",
    "    delta = 0\n",
    "    nsvf = np.zeros(shape=WORLD.grid.shape)\n",
    "    for s in WORLD.states:\n",
    "        v = svf[s.row, s.col]\n",
    "        acts = WORLD.get_possible_actions(s)\n",
    "        for a in acts:\n",
    "            ns = s(a)\n",
    "            nsvf[s.row, s.col] += 1 / len(acts) * (WORLD.get_reward(ns) + gamma * svf[ns.row, ns.col])\n",
    "        delta = max(delta, np.abs(v - nsvf[s.row, s.col]))\n",
    "    if delta < eps:\n",
    "        break\n",
    "    svf = nsvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ================= STATE VALUE FUNCTION =================\n",
      "           ........................................................\n",
      "     0     |  -73.21  | -103.65  |  -86.99  |  -75.56  |  -68.01  |\n",
      "           ........................................................\n",
      "     1     |  -59.04  |  -74.20  | -110.75  |  -96.88  |   0.00   |\n",
      "           ........................................................\n",
      "     2     |  -49.38  |  -56.35  |   0.00   | -136.61  | -126.97  |\n",
      "           ........................................................\n",
      "     3     |  -49.21  |  -64.25  | -117.16  | -120.42  | -145.54  |\n",
      "           ........................................................\n",
      "     4     |  -50.42  |  -62.83  |  -94.76  | -135.88  | -126.64  |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "print_grid(svf, additional_format=\".2f\", header=\"STATE VALUE FUNCTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LearningAgent:\n",
    "    def __init__(self, world, initial_state, actionChoosePolicy):\n",
    "        self.world = world\n",
    "        self.state = initial_state\n",
    "        self.actionChooser = actionChoosePolicy\n",
    "        # action value function\n",
    "        self.Q = {s:{k: 0 for k in world.get_possible_actions(s)} for s in world.states}\n",
    "\n",
    "    \n",
    "    def do_learn_step(self):\n",
    "        pass\n",
    "        \n",
    "    def get_live_policy(self):\n",
    "        return {state: max(self.Q[state], key=lambda a: self.Q[state][a]) for state in self.Q.keys()}\n",
    "    \n",
    "    def get_action_value_fun(self):\n",
    "        return self.Q\n",
    "    \n",
    "    def print_action_value(self):\n",
    "        for a in self.world.get_all_actions():\n",
    "            qq = {state: self.Q[state][a] if a in self.world.get_possible_actions(state) else -np.inf for state in self.Q.keys()}\n",
    "            mat = self.world.state_f_to_mat(qq, none_obj=0)\n",
    "            print_grid(mat, header=name_by_act(a), additional_format=\".3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning with eps-greedy walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsGreedyPolicy(eps):\n",
    "    def getAction(state, possible_actions, Q):\n",
    "        if np.random.uniform() < eps:\n",
    "            act = possible_actions[np.random.choice(len(possible_actions))]\n",
    "        else:\n",
    "            act = max(possible_actions, key=lambda a: Q[state][a])\n",
    "        return act\n",
    "    return getAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(LearningAgent):\n",
    "    def __init__(self, world, initial_state, actionChoosePolicy, gamma, alpha):\n",
    "        LearningAgent.__init__(self, world, initial_state, actionChoosePolicy)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    \n",
    "    def do_learn_step(self):\n",
    "        possible_acts = self.world.get_possible_actions(self.state) # get possible actions\n",
    "        act = self.actionChooser(self.state, possible_acts, self.Q) # choose action with policy\n",
    "        new_state = self.state(act) # next agent state\n",
    "        reward = self.world.get_reward(new_state) # reward for action\n",
    "        \n",
    "        oldQsa = self.Q[self.state][act]\n",
    "        max_aQns = max(self.Q[new_state][a] for a in self.world.get_possible_actions(new_state))\n",
    "        newQsa = oldQsa + self.alpha * (reward + self.gamma * max_aQns - oldQsa)\n",
    "        self.Q[self.state][act] = newQsa\n",
    "\n",
    "        # do not forget to enter new state!\n",
    "        self.state = new_state\n",
    "        \n",
    "        return np.abs(newQsa - oldQsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def do_q_learning(eps, gamma, alpha):\n",
    "    # initializing world\n",
    "    random_start_state = WORLD.states[np.random.choice(len(WORLD.states))]\n",
    "    agent = QLearningAgent(WORLD, random_start_state, epsGreedyPolicy(eps), gamma, alpha)\n",
    "    \n",
    "    nb_iters = 0\n",
    "    while True:\n",
    "        delta = agent.do_learn_step()\n",
    "        nb_iters += 1\n",
    "        if nb_iters > 10000:\n",
    "            break\n",
    "        \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_agent = do_q_learning(eps=0.7, gamma=0.9, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ........................................................\n",
      "     0     |    ðŸ¡¢     |    ðŸ¡£     |    ðŸ¡¢     |    ðŸ¡¢     |    ðŸ¡      |\n",
      "           ........................................................\n",
      "     1     |    ðŸ¡¢     |    ðŸ¡¢     |    ðŸ¡¢     |    ðŸ¡¡     |    âœ–     |\n",
      "           ........................................................\n",
      "     2     |    ðŸ¡¢     |    ðŸ¡¡     |    âœ–     |    ðŸ¡¡     |    ðŸ¡      |\n",
      "           ........................................................\n",
      "     3     |    ðŸ¡¡     |    ðŸ¡¡     |    ðŸ¡      |    ðŸ¡¡     |    ðŸ¡¡     |\n",
      "           ........................................................\n",
      "     4     |    ðŸ¡¡     |    ðŸ¡¡     |    ðŸ¡      |    ðŸ¡¢     |    ðŸ¡¡     |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "opt_policy = final_agent.get_live_policy()\n",
    "acts = WORLD.state_f_to_mat(opt_policy, none_obj=IMPOSSIBLE_ACTION)\n",
    "symbolic_acts = acts_array_to_sym(acts)\n",
    "print_grid(symbolic_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt_policy = final_agent.get_live_policy()\n",
    "import copy\n",
    "# state value function\n",
    "state_v_fun = {s: 0 for s in WORLD.states}\n",
    "eps = 1e-3\n",
    "gamma = 0.5\n",
    "while True:\n",
    "    delta = 0\n",
    "    new_svf = {s: 0 for s in WORLD.states}\n",
    "    for s in WORLD.states:\n",
    "        v = state_v_fun[s]\n",
    "        action = opt_policy[s]\n",
    "        ns = s(action)\n",
    "        new_svf[s] += (WORLD.get_reward(ns) + gamma * state_v_fun[ns])\n",
    "        delta = max(delta, np.abs(v - new_svf[s]))\n",
    "    state_v_fun = new_svf\n",
    "    if delta < eps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ........................................................\n",
      "     0     |   4.17   |   8.33   |  66.67   |  133.33  |  66.67   |\n",
      "           ........................................................\n",
      "     1     |   8.33   |  16.67   |  33.33   |  66.67   |   inf    |\n",
      "           ........................................................\n",
      "     2     |   4.17   |   8.33   |   inf    |  33.33   |  16.67   |\n",
      "           ........................................................\n",
      "     3     |   2.08   |   4.17   |   2.08   |  16.67   |   8.33   |\n",
      "           ........................................................\n",
      "     4     |   1.04   |   2.08   |   1.04   |   2.08   |   4.17   |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "print_grid(WORLD.state_f_to_mat(state_v_fun, none_obj=np.inf), additional_format=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ........................................................\n",
      "     0     |   0.0    |   0.0    |  -100.0  |   0.0    |  100.0   |\n",
      "           ........................................................\n",
      "     1     |   0.0    |   0.0    |   0.0    |   0.0    |   inf    |\n",
      "           ........................................................\n",
      "     2     |   0.0    |   0.0    |   inf    |   0.0    |   0.0    |\n",
      "           ........................................................\n",
      "     3     |   0.0    |   0.0    |   0.0    |  -100.0  |   0.0    |\n",
      "           ........................................................\n",
      "     4     |   0.0    |   0.0    |   0.0    |   0.0    |   0.0    |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "print_grid(WORLD.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoltzmannPolicy:\n",
    "    def __init__(self, T):\n",
    "        self.initT = T\n",
    "        self.curT = T\n",
    "        self.step = 2\n",
    "    \n",
    "    def __call__(self, state, possible_actions, Q):\n",
    "        self.curT = self.initT / np.log(self.step)\n",
    "        \n",
    "        probs = np.array([np.exp(Q[state][a] / self.curT) for a in possible_actions])\n",
    "        probs /= np.sum(probs)\n",
    "        idx = np.where(np.random.multinomial(1, probs)==1)[0][0]\n",
    "        \n",
    "        self.step += 1\n",
    "        return possible_actions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SarsaLearningAgent(LearningAgent):\n",
    "    def __init__(self, world, initial_state, actionChoosePolicy, gamma, alpha):\n",
    "        LearningAgent.__init__(self, world, initial_state, actionChoosePolicy)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.act = self.actionChooser(self.state, \n",
    "                                      self.world.get_possible_actions(self.state),\n",
    "                                      self.Q)\n",
    "\n",
    "    def reenter(self, new_state):\n",
    "        self.state = new_state\n",
    "        self.act = self.actionChooser(self.state, \n",
    "                                      self.world.get_possible_actions(self.state),\n",
    "                                      self.Q)\n",
    "    \n",
    "    def do_learn_step(self):\n",
    "        new_state = self.state(self.act) # next agent state\n",
    "        reward = self.world.get_reward(new_state) # reward for action   \n",
    "        possible_acts = self.world.get_possible_actions(new_state)\n",
    "        \n",
    "        act_new = self.actionChooser(new_state, possible_acts, self.Q)\n",
    "        oldQsa = self.Q[self.state][self.act]\n",
    "        newQsa = oldQsa + self.alpha * (reward + self.gamma * self.Q[new_state][act_new] - oldQsa)\n",
    "        self.Q[self.state][self.act] = newQsa\n",
    "        \n",
    "        # do not forget to enter new state!\n",
    "        self.state = new_state\n",
    "        self.act = act_new\n",
    "        \n",
    "        return np.abs(newQsa - oldQsa)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "def do_sarsa_learning(T, gamma, alpha):\n",
    "    # initializing world\n",
    "    policy = BoltzmannPolicy(T)\n",
    "#     policy = epsGreedyPolicy(0.2)\n",
    "    \n",
    "    agent = SarsaLearningAgent(WORLD, WORLD.states[0], policy, gamma, alpha)\n",
    "    \n",
    "    N = 1000\n",
    "    M = 1000\n",
    "    with tqdm(range(N * M)) as timer:\n",
    "        for i in range(N):\n",
    "            random_start_state = WORLD.states[np.random.choice(len(WORLD.states))]\n",
    "            agent.reenter(random_start_state)\n",
    "            for j in range(M):\n",
    "                agent.do_learn_step()\n",
    "                timer.update()\n",
    "        \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ........................................................\n",
      "     0     |   0.0    |   0.0    |  -100.0  |   0.0    |  100.0   |\n",
      "           ........................................................\n",
      "     1     |   0.0    |   0.0    |   0.0    |   0.0    |   inf    |\n",
      "           ........................................................\n",
      "     2     |   0.0    |   0.0    |   inf    |   0.0    |   0.0    |\n",
      "           ........................................................\n",
      "     3     |   0.0    |   0.0    |   0.0    |  -100.0  |   0.0    |\n",
      "           ........................................................\n",
      "     4     |   0.0    |   0.0    |   0.0    |   0.0    |   0.0    |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "print_grid(WORLD.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [01:34<00:00, 10547.58it/s]\n"
     ]
    }
   ],
   "source": [
    "final_agent = do_sarsa_learning(T = 20, gamma=0.7, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ........................................................\n",
      "     0     |    ðŸ¡£     |    ðŸ¡£     |    ðŸ¡¢     |    ðŸ¡¢     |    ðŸ¡      |\n",
      "           ........................................................\n",
      "     1     |    ðŸ¡¢     |    ðŸ¡¢     |    ðŸ¡¢     |    ðŸ¡¡     |    âœ–     |\n",
      "           ........................................................\n",
      "     2     |    ðŸ¡¢     |    ðŸ¡¡     |    âœ–     |    ðŸ¡¡     |    ðŸ¡      |\n",
      "           ........................................................\n",
      "     3     |    ðŸ¡¡     |    ðŸ¡¡     |    ðŸ¡      |    ðŸ¡¡     |    ðŸ¡¡     |\n",
      "           ........................................................\n",
      "     4     |    ðŸ¡¡     |    ðŸ¡¡     |    ðŸ¡      |    ðŸ¡      |    ðŸ¡¡     |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "opt_policy = final_agent.get_live_policy()\n",
    "acts = WORLD.state_f_to_mat(opt_policy, none_obj=IMPOSSIBLE_ACTION)\n",
    "symbolic_acts = acts_array_to_sym(acts)\n",
    "print_grid(symbolic_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ========================== up ==========================\n",
      "           ........................................................\n",
      "     0     |   -inf   |   -inf   |   -inf   |   -inf   |   -inf   |\n",
      "           ........................................................\n",
      "     1     |  0.000   |  0.000   | -20.000  | 137.255  |  0.000   |\n",
      "           ........................................................\n",
      "     2     |  0.654   |  47.078  |  0.000   |  96.078  |   -inf   |\n",
      "           ........................................................\n",
      "     3     |  23.068  |  32.955  |   -inf   |  67.254  |  47.078  |\n",
      "           ........................................................\n",
      "     4     |  16.144  |  23.068  |  -0.898  | -20.000  |  32.950  |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n",
      "           ========================= down =========================\n",
      "           ........................................................\n",
      "     0     |  32.951  |  47.072  |  0.000   |  0.000   |   -inf   |\n",
      "           ........................................................\n",
      "     1     |  0.000   |  0.000   |   -inf   |  0.000   |  0.000   |\n",
      "           ........................................................\n",
      "     2     |  -0.000  |  0.000   |  0.000   | -20.000  |  0.000   |\n",
      "           ........................................................\n",
      "     3     |  0.000   |  -0.003  |  -0.006  |  0.000   |  -0.025  |\n",
      "           ........................................................\n",
      "     4     |   -inf   |   -inf   |   -inf   |   -inf   |   -inf   |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n",
      "           ======================== right =========================\n",
      "           ........................................................\n",
      "     0     |  0.011   | -20.000  | 137.252  | 196.078  |   -inf   |\n",
      "           ........................................................\n",
      "     1     |  47.078  |  67.255  |  96.078  |   -inf   |  0.000   |\n",
      "           ........................................................\n",
      "     2     |  32.955  |   -inf   |  0.000   |  0.000   |   -inf   |\n",
      "           ........................................................\n",
      "     3     |  0.381   |  -0.002  | -36.000  |  0.000   |   -inf   |\n",
      "           ........................................................\n",
      "     4     |  0.952   |  -0.129  |  -0.087  |  1.485   |   -inf   |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n",
      "           ========================= left =========================\n",
      "           ........................................................\n",
      "     0     |   -inf   |  0.000   |  1.281   |  0.000   | 137.255  |\n",
      "           ........................................................\n",
      "     1     |   -inf   |  0.000   |  0.000   |  0.000   |  0.000   |\n",
      "           ........................................................\n",
      "     2     |   -inf   |  0.000   |  0.000   |   -inf   |  67.255  |\n",
      "           ........................................................\n",
      "     3     |   -inf   |  0.000   |  23.067  |  0.000   | -20.000  |\n",
      "           ........................................................\n",
      "     4     |   -inf   |  -0.000  |  16.148  |  11.297  |  -0.136  |\n",
      "           ........................................................\n",
      "                0          1          2          3          4     \n"
     ]
    }
   ],
   "source": [
    "final_agent.print_action_value()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
