# Reinforcement learning (обучение с подкреплением)

## Agent and Environment

Агент взаимодейтсвует со средой в каждый момент времени `t = 0, 1, 2, ...`.
В момент времени `t` агент получает от среды (world, environment) 
состояние `S_t` из множества возможных состояний `S`. Исходя из состояния 
агент выбирает действие (action) `A_t` из моножества возможных действий в
этом состоянии `A(s_t)`. После выполнения действия агент получает от серды
награду `R_{t+1}` и получает новое состояние `S_{t+1}` от среды.

В каждый момент времени `t` агент следует политике `π_t(a|s)` -- это 
вероятность того, что `A_t = a` при `S_t = s`. Разные методы обучения с
подкреплением по-разному "обучают" политику, т.е. политика `π_t(a|s)`
меняется со временм.

В начальный момент времени `t = 0` награда `R_0 = 0`, как я понимаю (это не важно).

## Returns

Общая задача -- максимизация суммарной награды, полученной за "длительное" взаимодействие агента со средой. Если мы уверены, что задача решится агентом за `T` шагов, то вводят:

* `G_t = R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T`

Это и есть *return* -- суммарная награда после момента времени `t`.
**Хотим**: максимизировать математическое ожидание `G_t`...

Для задач, в которых агент может блуждать по миру, вообще говоря, бесконечно, этот *return* вводят чуть по-другому:

* `G_t = R_{t+1} + γ * R_{t+2} + γ^2 * R_{t+3} + ...`

Тут `0 <= γ <= 1` -- это *discount rate*. Смысл понятем: нас больше
привлекает награда в "ближайшее" время. Так же матан нам говорит, что
если `γ < 1` и `|R_k| < ∞`, то ряд сходится. Если `γ = 0`, то агента
называют близоруким.

 
## Markov property

Будем считать, что состояние мира `S_t` в момент времени `t` и значение
награды `R_t` в этот момент времни подчиняются свойству Маркова 
(Markov property), т.е. зависят исключительно от `S_{t-1}` и действия `A_{t-1}`, т.е.:

`p(s', r | s, a) = Pr[S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a]`

## MDP (Markov decision processes)

*NB.* говорим везде о конечном числе состояний `S_t`.

* [ТУТ](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf) (стр. 60)


